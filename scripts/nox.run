#!/usr/bin/env bash
set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
REPO_ROOT="$(cd "${SCRIPT_DIR}/.." && pwd)"

VENV_DIR="${REPO_ROOT}/.venv"
if [[ ! -d "${VENV_DIR}" ]]; then
  python -m venv "${VENV_DIR}"
fi
source "${VENV_DIR}/bin/activate"

pip install --quiet --upgrade pip >/dev/null
pip install --quiet -r "${REPO_ROOT}/requirements.txt" >/dev/null

MODEL_NAME=${MODEL_NAME:-noxllm-05b:latest}
DEFAULT_ENDPOINT="http://127.0.0.1:11434/api/generate"
CENTRAL_LLM_URL=${CENTRAL_LLM_URL_OVERRIDE:-$DEFAULT_ENDPOINT}
CENTRAL_LLM_MODEL=${CENTRAL_LLM_MODEL_OVERRIDE:-$MODEL_NAME}
export CENTRAL_LLM_URL
export CENTRAL_LLM_MODEL

OLLAMA_BIN="${REPO_ROOT}/inference/ollama"
if [[ ! -x "$OLLAMA_BIN" ]]; then
  echo "error: expected Ollama binary at $OLLAMA_BIN" >&2
  exit 1
fi

export OLLAMA_MODELS="${REPO_ROOT}/models"
mkdir -p "$OLLAMA_MODELS"

start_ollama() {
  export OLLAMA_HOST=${OLLAMA_HOST:-127.0.0.1:11434}
  "$OLLAMA_BIN" serve >/dev/null 2>&1 &
  SERVER_PID=$!
  trap 'kill $SERVER_PID >/dev/null 2>&1 || true' EXIT

  for _ in {1..30}; do
    if curl -sSf "http://${OLLAMA_HOST}/api/version" >/dev/null 2>&1; then
      break
    fi
    sleep 1
  done

  if ! curl -sSf "http://${OLLAMA_HOST}/api/version" >/dev/null 2>&1; then
    echo "error: Ollama server did not start" >&2
    exit 1
  fi

  if ! "$OLLAMA_BIN" list | grep -q "^$MODEL_NAME"; then
    MANIFEST="$OLLAMA_MODELS/ModelFile"
    if [[ ! -f "$MANIFEST" ]]; then
      echo "error: no ModelFile found at $MANIFEST" >&2
      exit 1
    fi
    echo "Creating model $MODEL_NAME from $MANIFEST" >&2
    if ! "$OLLAMA_BIN" create "$MODEL_NAME" -f "$MANIFEST"; then
      echo "error: failed to create model $MODEL_NAME" >&2
      exit 1
    fi
  fi
}

start_ollama

echo "Inference endpoint: $CENTRAL_LLM_URL" >&2
echo "Inference model:    $CENTRAL_LLM_MODEL" >&2

python "${REPO_ROOT}/main.py" --stream --show-think --url "$CENTRAL_LLM_URL" --model "$CENTRAL_LLM_MODEL"
